{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification evaluvation report.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhIvp8mZ8pZCBv6UA3NGFu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/AI-Engineering/blob/master/07-Machine%20Learning/Classification_evaluvation_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGP8Wcu3gz37",
        "outputId": "f833abeb-71c0-4665-94da-7708ea7d898c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0\n"
          ]
        }
      ],
      "source": [
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        " \n",
        "# Test accuracy\n",
        "actual = [0,0,0,0,0,1,1,1,1,1]\n",
        "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
        "accuracy = accuracy_metric(actual, predicted)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Calculating and Displaying a Pretty Confusion Matrix\n",
        " \n",
        "# calculate a confusion matrix\n",
        "def confusion_matrix(actual, predicted):\n",
        "\tunique = set(actual)\n",
        "\tmatrix = [list() for x in range(len(unique))]\n",
        "\tfor i in range(len(unique)):\n",
        "\t\tmatrix[i] = [0 for x in range(len(unique))]\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tx = lookup[actual[i]]\n",
        "\t\ty = lookup[predicted[i]]\n",
        "\t\tmatrix[y][x] += 1\n",
        "\treturn unique, matrix\n",
        " \n",
        "# pretty print a confusion matrix\n",
        "def print_confusion_matrix(unique, matrix):\n",
        "\tprint('(A)' + ' '.join(str(x) for x in unique))\n",
        "\tprint('(P)---')\n",
        "\tfor i, x in enumerate(unique):\n",
        "\t\tprint(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))\n",
        " \n",
        "# Test confusion matrix with integers\n",
        "actual = [0,0,0,0,0,1,1,1,1,1]\n",
        "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
        "unique, matrix = confusion_matrix(actual, predicted)\n",
        "print_confusion_matrix(unique, matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR9MyK4Xg5Fk",
        "outputId": "ba4f3a9c-17d7-4a29-fff8-fdb106f61fa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(A)0 1\n",
            "(P)---\n",
            "0| 3 1\n",
            "1| 2 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean absolute error\n",
        "def mae_metric(actual, predicted):\n",
        "\tsum_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tsum_error += abs(predicted[i] - actual[i])\n",
        "\treturn sum_error / float(len(actual))\n",
        " \n",
        "# Test RMSE\n",
        "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
        "mae = mae_metric(actual, predicted)\n",
        "print(mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2OwgaHbhEvp",
        "outputId": "3b644255-2659-4a1d-927d-adfd9aad7952"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007999999999999993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        " \n",
        "# Calculate root mean squared error\n",
        "def rmse_metric(actual, predicted):\n",
        "\tsum_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tprediction_error = predicted[i] - actual[i]\n",
        "\t\tsum_error += (prediction_error ** 2)\n",
        "\tmean_error = sum_error / float(len(actual))\n",
        "\treturn sqrt(mean_error)\n",
        " \n",
        "# Test RMSE\n",
        "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
        "rmse = rmse_metric(actual, predicted)\n",
        "print(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQoyBMBChS1A",
        "outputId": "1d77a6e3-0d07-46ea-9ebb-0417b303f79b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00894427190999915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def error(df,col1,col2):\n",
        "    val=[]\n",
        "    for index, (value1, value2) in enumerate(zip(df[col1], df[col2])):\n",
        "        val.append(value1-value2)\n",
        "    return val\n",
        "    \n",
        "def absolute_error(df,col):\n",
        "    val=[]\n",
        "    for index,value in enumerate(df[col]):\n",
        "        val.append(abs(value))\n",
        "    return val\n",
        "\n",
        "def mean_sq_error(df,col):\n",
        "    return ss_res(df,col)/len(df[col])\n",
        "\n",
        "def mape(df,col1,col2):\n",
        "    val=sum(df[col1])/sum(df[col2])\n",
        "    return val\n",
        "\n",
        "def ss_res(df,col):\n",
        "    val=0\n",
        "    for index,value in enumerate(df[col]):\n",
        "        val=val+(value*value)\n",
        "    return val\n",
        "\n",
        "def ss_tot(df,col):\n",
        "    val=0\n",
        "    mean_val=data_d['y'].mean()\n",
        "    for index,value in enumerate(df[col]):\n",
        "        val=val+ (value-mean_val)*(value-mean_val)\n",
        "    return val\n",
        "        "
      ],
      "metadata": {
        "id": "d74NMHI9hYtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Source: http://ethen8181.github.io/machine-learning/model_selection/auc/auc.html#Implementation\n",
        "def _binary_clf_curve(y_true, y_score):\n",
        "    \"\"\"\n",
        "    Calculate true and false positives per binary classification\n",
        "    threshold (can be used for roc curve or precision/recall curve);\n",
        "    the calcuation makes the assumption that the positive case\n",
        "    will always be labeled as 1\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : 1d ndarray, shape = [n_samples]\n",
        "        True targets/labels of binary classification\n",
        "    y_score : 1d ndarray, shape = [n_samples]\n",
        "        Estimated probabilities or scores\n",
        "    Returns\n",
        "    -------\n",
        "    tps : 1d ndarray\n",
        "        True positives counts, index i records the number\n",
        "        of positive samples that got assigned a\n",
        "        score >= thresholds[i].\n",
        "        The total number of positive samples is equal to\n",
        "        tps[-1] (thus false negatives are given by tps[-1] - tps)\n",
        "    fps : 1d ndarray\n",
        "        False positives counts, index i records the number\n",
        "        of negative samples that got assigned a\n",
        "        score >= thresholds[i].\n",
        "        The total number of negative samples is equal to\n",
        "        fps[-1] (thus true negatives are given by fps[-1] - fps)\n",
        "    thresholds : 1d ndarray\n",
        "        Predicted score sorted in decreasing order\n",
        "    References\n",
        "    ----------\n",
        "    Github: scikit-learn _binary_clf_curve\n",
        "    - https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/metrics/ranking.py#L263\n",
        "    \"\"\"\n",
        "\n",
        "    # sort predicted scores in descending order\n",
        "    # and also reorder corresponding truth values\n",
        "    desc_score_indices = np.argsort(y_score)[::-1]\n",
        "    y_score = y_score[desc_score_indices]\n",
        "    y_true = y_true[desc_score_indices]\n",
        "\n",
        "    # y_score typically consists of tied values. Here we extract\n",
        "    # the indices associated with the distinct values. We also\n",
        "    # concatenate a value for the end of the curve\n",
        "    distinct_indices = np.where(np.diff(y_score))[0]\n",
        "    end = np.array([y_true.size - 1])\n",
        "    threshold_indices = np.hstack((distinct_indices, end))\n",
        "\n",
        "    thresholds = y_score[threshold_indices]\n",
        "    tps = np.cumsum(y_true)[threshold_indices]\n",
        "\n",
        "    # (1 + threshold_indices) = the number of positives\n",
        "    # at each index, thus number of data points minus true\n",
        "    # positives = false positives\n",
        "    fps = (1 + threshold_indices) - tps\n",
        "    return tps, fps, thresholds\n",
        "\n",
        "\n",
        "def _roc_auc_score(y_true, y_score):\n",
        "    \"\"\"\n",
        "    Compute Area Under the Curve (AUC) from prediction scores\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : 1d ndarray, shape = [n_samples]\n",
        "        True targets/labels of binary classification\n",
        "    y_score : 1d ndarray, shape = [n_samples]\n",
        "        Estimated probabilities or scores\n",
        "    Returns\n",
        "    -------\n",
        "    auc : float\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure the target is binary\n",
        "    if np.unique(y_true).size != 2:\n",
        "        raise ValueError('Only two class should be present in y_true. ROC AUC score '\n",
        "                         'is not defined in that case.')\n",
        "\n",
        "    tps, fps, _ = _binary_clf_curve(y_true, y_score)\n",
        "\n",
        "    # convert count to rate\n",
        "    tpr = tps / tps[-1]\n",
        "    fpr = fps / fps[-1]\n",
        "\n",
        "    # compute AUC using the trapezoidal rule;\n",
        "    # appending an extra 0 is just to ensure the length matches\n",
        "    zero = np.array([0])\n",
        "    tpr_diff = np.hstack((np.diff(tpr), zero))\n",
        "    fpr_diff = np.hstack((np.diff(fpr), zero))\n",
        "    auc = np.dot(tpr, fpr_diff) + np.dot(tpr_diff, fpr_diff) / 2\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    y_true = np.array([0, 0, 1, 1])\n",
        "    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        "    print(_roc_auc_score(y_true, y_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_HJbTCVg0uF",
        "outputId": "c6ad39e3-ae15-4a78-88b5-1aa4dcd9e002"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: http://ethen8181.github.io/machine-learning/model_selection/auc/auc.html#Implementation\n",
        "def _binary_clf_curve(y_true, y_score):\n",
        "    \"\"\"\n",
        "    Calculate true and false positives per binary classification\n",
        "    threshold (can be used for roc curve or precision/recall curve);\n",
        "    the calcuation makes the assumption that the positive case\n",
        "    will always be labeled as 1\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : 1d ndarray, shape = [n_samples]\n",
        "        True targets/labels of binary classification\n",
        "    y_score : 1d ndarray, shape = [n_samples]\n",
        "        Estimated probabilities or scores\n",
        "    Returns\n",
        "    -------\n",
        "    tps : 1d ndarray\n",
        "        True positives counts, index i records the number\n",
        "        of positive samples that got assigned a\n",
        "        score >= thresholds[i].\n",
        "        The total number of positive samples is equal to\n",
        "        tps[-1] (thus false negatives are given by tps[-1] - tps)\n",
        "    fps : 1d ndarray\n",
        "        False positives counts, index i records the number\n",
        "        of negative samples that got assigned a\n",
        "        score >= thresholds[i].\n",
        "        The total number of negative samples is equal to\n",
        "        fps[-1] (thus true negatives are given by fps[-1] - fps)\n",
        "    thresholds : 1d ndarray\n",
        "        Predicted score sorted in decreasing order\n",
        "    References\n",
        "    ----------\n",
        "    Github: scikit-learn _binary_clf_curve\n",
        "    - https://github.com/scikit-learn/scikit-learn/blob/ab93d65/sklearn/metrics/ranking.py#L263\n",
        "    \"\"\"\n",
        "\n",
        "    # sort predicted scores in descending order\n",
        "    # and also reorder corresponding truth values\n",
        "    desc_score_indices = np.argsort(y_score)[::-1]\n",
        "    y_score = y_score[desc_score_indices]\n",
        "    y_true = y_true[desc_score_indices]\n",
        "\n",
        "    # y_score typically consists of tied values. Here we extract\n",
        "    # the indices associated with the distinct values. We also\n",
        "    # concatenate a value for the end of the curve\n",
        "    distinct_indices = np.where(np.diff(y_score))[0]\n",
        "    end = np.array([y_true.size - 1])\n",
        "    threshold_indices = np.hstack((distinct_indices, end))\n",
        "\n",
        "    thresholds = y_score[threshold_indices]\n",
        "    tps = np.cumsum(y_true)[threshold_indices]\n",
        "\n",
        "    # (1 + threshold_indices) = the number of positives\n",
        "    # at each index, thus number of data points minus true\n",
        "    # positives = false positives\n",
        "    fps = (1 + threshold_indices) - tps\n",
        "    return tps, fps, thresholds\n",
        "\n",
        "\n",
        "def _roc_auc_score(y_true, y_score):\n",
        "    \"\"\"\n",
        "    Compute Area Under the Curve (AUC) from prediction scores\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : 1d ndarray, shape = [n_samples]\n",
        "        True targets/labels of binary classification\n",
        "    y_score : 1d ndarray, shape = [n_samples]\n",
        "        Estimated probabilities or scores\n",
        "    Returns\n",
        "    -------\n",
        "    auc : float\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure the target is binary\n",
        "    if np.unique(y_true).size != 2:\n",
        "        raise ValueError('Only two class should be present in y_true. ROC AUC score '\n",
        "                         'is not defined in that case.')\n",
        "\n",
        "    tps, fps, _ = _binary_clf_curve(y_true, y_score)\n",
        "\n",
        "    # convert count to rate\n",
        "    tpr = tps / tps[-1]\n",
        "    fpr = fps / fps[-1]\n",
        "\n",
        "    # compute AUC using the trapezoidal rule;\n",
        "    # appending an extra 0 is just to ensure the length matches\n",
        "    zero = np.array([0])\n",
        "    tpr_diff = np.hstack((np.diff(tpr), zero))\n",
        "    fpr_diff = np.hstack((np.diff(fpr), zero))\n",
        "    auc = np.dot(tpr, fpr_diff) + np.dot(tpr_diff, fpr_diff) / 2\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    y_true = np.array([0, 0, 1, 1])\n",
        "    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        "    print(_roc_auc_score(y_true, y_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idbgEnfwjBcG",
        "outputId": "b988bf00-e3f1-4988-d52a-46d62664b3e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tied_rank(x):\n",
        "    \"\"\"\n",
        "    Computes the tied rank of elements in x.\n",
        "    This function computes the tied rank of elements in x.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : list of numbers, numpy array\n",
        "    Returns\n",
        "    -------\n",
        "    score : list of numbers\n",
        "            The tied rank f each element in x\n",
        "    \"\"\"\n",
        "    sorted_x = sorted(zip(x,range(len(x))))\n",
        "    r = [0 for k in x]\n",
        "    cur_val = sorted_x[0][0]\n",
        "    last_rank = 0\n",
        "    for i in range(len(sorted_x)):\n",
        "        if cur_val != sorted_x[i][0]:\n",
        "            cur_val = sorted_x[i][0]\n",
        "            for j in range(last_rank, i): \n",
        "                r[sorted_x[j][1]] = float(last_rank+1+i)/2.0\n",
        "            last_rank = i\n",
        "        if i==len(sorted_x)-1:\n",
        "            for j in range(last_rank, i+1): \n",
        "                r[sorted_x[j][1]] = float(last_rank+i+2)/2.0\n",
        "    return r"
      ],
      "metadata": {
        "id": "5yoGujgejFdA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auc(actual, posterior):\n",
        "    \"\"\"\n",
        "    Computes the area under the receiver-operater characteristic (AUC)\n",
        "    This function computes the AUC error metric for binary classification.\n",
        "    Parameters\n",
        "    ----------\n",
        "    actual : list of binary numbers, numpy array\n",
        "             The ground truth value\n",
        "    posterior : same type as actual\n",
        "                Defines a ranking on the binary numbers, from most likely to\n",
        "                be positive to least likely to be positive.\n",
        "    Returns\n",
        "    -------\n",
        "    score : double\n",
        "            The mean squared error between actual and posterior\n",
        "    \"\"\"\n",
        "    r = tied_rank(posterior)\n",
        "    num_positive = len([0 for x in actual if x==1])\n",
        "    num_negative = len(actual)-num_positive\n",
        "    sum_positive = sum([r[i] for i in range(len(r)) if actual[i]==1])\n",
        "    auc = ((sum_positive - num_positive*(num_positive+1)/2.0) /\n",
        "           (num_negative*num_positive))\n",
        "    return auc"
      ],
      "metadata": {
        "id": "oQ_MHFbRjJWU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the kappa\n",
        "    kappa calculates the kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            if i == j:\n",
        "                d = 0.0\n",
        "            else:\n",
        "                d = 1.0\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "metadata": {
        "id": "ns_S6-0wjqSJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the linear weighted kappa\n",
        "    linear_weighted_kappa calculates the linear weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    linear_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    linear_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = abs(i - j) / float(num_ratings - 1)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "metadata": {
        "id": "-dDn_cR4jco6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the confusion matrix between rater's ratings\n",
        "    \"\"\"\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(rater_a + rater_b)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(rater_a + rater_b)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    conf_mat = [[0 for i in range(num_ratings)]\n",
        "                for j in range(num_ratings)]\n",
        "    for a, b in zip(rater_a, rater_b):\n",
        "        conf_mat[a - min_rating][b - min_rating] += 1\n",
        "    return conf_mat"
      ],
      "metadata": {
        "id": "kfIijZdDjMIH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram(ratings, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Returns the counts of each type of rating that a rater made\n",
        "    \"\"\"\n",
        "    if min_rating is None:\n",
        "        min_rating = min(ratings)\n",
        "    if max_rating is None:\n",
        "        max_rating = max(ratings)\n",
        "    num_ratings = int(max_rating - min_rating + 1)\n",
        "    hist_ratings = [0 for x in range(num_ratings)]\n",
        "    for r in ratings:\n",
        "        hist_ratings[r - min_rating] += 1\n",
        "    return hist_ratings"
      ],
      "metadata": {
        "id": "EqcutAiOj2L4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
        "    \"\"\"\n",
        "    Calculates the quadratic weighted kappa\n",
        "    quadratic_weighted_kappa calculates the quadratic weighted kappa\n",
        "    value, which is a measure of inter-rater agreement between two raters\n",
        "    that provide discrete numeric ratings.  Potential values range from -1\n",
        "    (representing complete disagreement) to 1 (representing complete\n",
        "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
        "    chance.\n",
        "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
        "    each correspond to a list of integer ratings.  These lists must have the\n",
        "    same length.\n",
        "    The ratings should be integers, and it is assumed that they contain\n",
        "    the complete range of possible ratings.\n",
        "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
        "    is the minimum possible rating, and max_rating is the maximum possible\n",
        "    rating\n",
        "    \"\"\"\n",
        "    rater_a = np.array(rater_a, dtype=int)\n",
        "    rater_b = np.array(rater_b, dtype=int)\n",
        "    assert(len(rater_a) == len(rater_b))\n",
        "    if min_rating is None:\n",
        "        min_rating = min(min(rater_a), min(rater_b))\n",
        "    if max_rating is None:\n",
        "        max_rating = max(max(rater_a), max(rater_b))\n",
        "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
        "                                min_rating, max_rating)\n",
        "    num_ratings = len(conf_mat)\n",
        "    num_scored_items = float(len(rater_a))\n",
        "\n",
        "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
        "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
        "\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "\n",
        "    for i in range(num_ratings):\n",
        "        for j in range(num_ratings):\n",
        "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
        "                              / num_scored_items)\n",
        "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
        "            numerator += d * conf_mat[i][j] / num_scored_items\n",
        "            denominator += d * expected_count / num_scored_items\n",
        "\n",
        "    return 1.0 - numerator / denominator"
      ],
      "metadata": {
        "id": "U-3GtOzEjxXG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_quadratic_weighted_kappa(kappas, weights=None):\n",
        "    \"\"\"\n",
        "    Calculates the mean of the quadratic\n",
        "    weighted kappas after applying Fisher's r-to-z transform, which is\n",
        "    approximately a variance-stabilizing transformation.  This\n",
        "    transformation is undefined if one of the kappas is 1.0, so all kappa\n",
        "    values are capped in the range (-0.999, 0.999).  The reverse\n",
        "    transformation is then applied before returning the result.\n",
        "    mean_quadratic_weighted_kappa(kappas), where kappas is a vector of\n",
        "    kappa values\n",
        "    mean_quadratic_weighted_kappa(kappas, weights), where weights is a vector\n",
        "    of weights that is the same size as kappas.  Weights are applied in the\n",
        "    z-space\n",
        "    \"\"\"\n",
        "    kappas = np.array(kappas, dtype=float)\n",
        "    if weights is None:\n",
        "        weights = np.ones(np.shape(kappas))\n",
        "    else:\n",
        "        weights = weights / np.mean(weights)\n",
        "\n",
        "    # ensure that kappas are in the range [-.999, .999]\n",
        "    kappas = np.array([min(x, .999) for x in kappas])\n",
        "    kappas = np.array([max(x, -.999) for x in kappas])\n",
        "\n",
        "    z = 0.5 * np.log((1 + kappas) / (1 - kappas)) * weights\n",
        "    z = np.mean(z)\n",
        "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)"
      ],
      "metadata": {
        "id": "RfJp7cmQjUhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_mean_quadratic_weighted_kappa(solution, submission):\n",
        "    predicted_score = submission[submission.columns[-1]].copy()\n",
        "    predicted_score.name = \"predicted_score\"\n",
        "    if predicted_score.index[0] == 0:\n",
        "        predicted_score = predicted_score[:len(solution)]\n",
        "        predicted_score.index = solution.index\n",
        "    combined = solution.join(predicted_score, how=\"left\")\n",
        "    groups = combined.groupby(by=\"essay_set\")\n",
        "    kappas = [quadratic_weighted_kappa(group[1][\"essay_score\"], group[1][\"predicted_score\"]) for group in groups]\n",
        "    weights = [group[1][\"essay_weight\"].irow(0) for group in groups]\n",
        "    return mean_quadratic_weighted_kappa(kappas, weights=weights)"
      ],
      "metadata": {
        "id": "ZeZEWQxxjRmU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}