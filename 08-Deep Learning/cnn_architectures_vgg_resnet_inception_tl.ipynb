{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/AI-Engineering/blob/master/08-Deep%20Learning/cnn_architectures_vgg_resnet_inception_tl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "G4pswaktw318"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN Architectures : VGG, Resnet, InceptionNet, XceptionNet \n",
        "\n",
        "### UseCases : Image Feature Extraction + Transfer Learning\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "A Gold mine dataset for comuter vision is the ImageNet dataset. It consists of about 14 M hand-labelled annotated images which contains over 22,000 day-to-day categories. Every year ImageNet competition is hosted in which the smaller version of this dataset (with 1000 categories) is used with an aim to accurately classify the images. Many winning solutions of the ImageNet Challenge have used state of the art convolutional neural network architectures to beat the best possible accuracy thresholds. In this kernel, I have discussed these popular architectures such as VGG16, 19, ResNet, AlexNet etc. In the end, I have explained how to generate image features using pretrained models and use them in machine learning models. \n",
        "\n",
        "## Contents \n",
        "\n",
        "<br>\n",
        "\n",
        "From the high level perspective, I have discussed three main components \n",
        "\n",
        "<ul>\n",
        "    <li>1. CNN Architectures   </li>\n",
        "<ul>\n",
        "    <li>1. 1 VGG16</li>\n",
        "    <li>1.2 VGG19 </li>\n",
        "    <li>1.3 InceptionNet</li>\n",
        "    <li>1.4 Resnet </li>\n",
        "    <li>1.5 XceptionNet</li>\n",
        "</ul>\n",
        "    <li>2. Image Feature Extraction  </li>\n",
        "    <li>3. Transfer Learning  </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "## 1. CNN Architectures\n",
        "## 1.1 &nbsp;&nbsp; VGG16 \n",
        "\n",
        "VGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:   \n",
        "\n",
        "1. This network contains total 16 layers in which weights and bias parameters are learnt.    \n",
        "2. A total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.     \n",
        "3. The number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).     \n",
        "4. The informative features are obtained by max pooling layers applied at different steps in the architecture.    \n",
        "5. The dense layers comprises of 4096, 4096, and 1000 nodes each.   \n",
        "6. The cons of this architecture are that it is slow to train and produces the model with very large size.   \n",
        "\n",
        "The VGG16 architecture is given below: \n",
        "\n",
        "![](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png)\n",
        "\n",
        "## Implementation : VGG16\n",
        "Let's see how we can create this architecture using python's keras library. The following code block shows the implementation of VGG16 in keras. \n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8e3ec93ecb551f460cc3dad2a48df7b8399a9c9",
        "id": "NWG_bpBJw31-"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Model\n",
        "\n",
        "_input = Input((224,224,1)) \n",
        "\n",
        "conv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(_input)\n",
        "conv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n",
        "pool1  = MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "conv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
        "conv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n",
        "pool2  = MaxPooling2D((2, 2))(conv4)\n",
        "\n",
        "conv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
        "conv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n",
        "conv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv6)\n",
        "pool3  = MaxPooling2D((2, 2))(conv7)\n",
        "\n",
        "conv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
        "conv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv8)\n",
        "conv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n",
        "pool4  = MaxPooling2D((2, 2))(conv10)\n",
        "\n",
        "conv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
        "conv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv11)\n",
        "conv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv12)\n",
        "pool5  = MaxPooling2D((2, 2))(conv13)\n",
        "\n",
        "flat   = Flatten()(pool5)\n",
        "dense1 = Dense(4096, activation=\"relu\")(flat)\n",
        "dense2 = Dense(4096, activation=\"relu\")(dense1)\n",
        "output = Dense(1000, activation=\"softmax\")(dense2)\n",
        "\n",
        "vgg16_model  = Model(inputs=_input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "42faa0d2e466c898a2dafaeb92295a95ac10583e",
        "id": "FpY6AGYHw32A"
      },
      "cell_type": "markdown",
      "source": [
        "## PreTrained Model : VGG16\n",
        "\n",
        "\n",
        "\n",
        "Keras library also provides the pre-trained model in which one can load the saved model weights, and use them for different purposes : transfer learning, image feature extraction, and object detection. We can load the model architecture given in the library, and then add all the weights to the respective layers. \n",
        "\n",
        "Before using the pretrained models, lets write a few functions which will be used to make some predictions. First, load some images and preprocess them. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99d88f29ec0337fbcf9c778a0bceaa92dca4ee04",
        "id": "C1jPd8ZPw32B"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image \n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import os \n",
        "\n",
        "img1 = \"../input/dogs-vs-cats-redux-kernels-edition/train/cat.11679.jpg\"\n",
        "img2 = \"../input/dogs-vs-cats-redux-kernels-edition/train/dog.2811.jpg\"\n",
        "img3 = \"../input/flowers-recognition/flowers/flowers/sunflower/7791014076_07a897cb85_n.jpg\"\n",
        "img4 = \"../input/fruits/fruits-360_dataset/fruits-360/Training/Banana/254_100.jpg\"\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "def _load_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img \n",
        "\n",
        "def _get_predictions(_model):\n",
        "    f, ax = plt.subplots(1, 4)\n",
        "    f.set_size_inches(80, 40)\n",
        "    for i in range(4):\n",
        "        ax[i].imshow(Image.open(imgs[i]).resize((200, 200), Image.ANTIALIAS))\n",
        "    plt.show()\n",
        "    \n",
        "    f, axes = plt.subplots(1, 4)\n",
        "    f.set_size_inches(80, 20)\n",
        "    for i,img_path in enumerate(imgs):\n",
        "        img = _load_image(img_path)\n",
        "        preds  = decode_predictions(_model.predict(img), top=3)[0]\n",
        "        b = sns.barplot(y=[c[1] for c in preds], x=[c[2] for c in preds], color=\"gray\", ax=axes[i])\n",
        "        b.tick_params(labelsize=55)\n",
        "        f.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f3387ea97c3ad83a7a5d8ebc650e2a42f493066",
        "id": "jtI5zZefw32B"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can perform following steps : \n",
        "1. import VGG16 architecture from keras.applications  \n",
        "2. Add the saved weights to the architecture \n",
        "3. Use model to perform predictions "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b003c6f9d5fc09f142c6cff8714e165d3a542fbe",
        "id": "pne-QVvJw32B"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "vgg16_weights = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "vgg16_model = VGG16(weights=vgg16_weights)\n",
        "_get_predictions(vgg16_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "54c5c8da0c9d25d923ad94ceef7afd140dcfa97f",
        "id": "T8xwImZXw32C"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 &nbsp;&nbsp; VGG19 \n",
        "\n",
        "VGG19 is a similar model architecure as VGG16 with three additional convolutional layers, it consists of a total of 16 Convolution layers and 3 dense layers.  Following is the architecture of VGG19 model. In VGG networks, the use of 3 x 3 convolutions with stride 1 gives an effective receptive filed equivalent to 7 * 7. This means there are fewer parameters to train. \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*cufAO77aeSWdShs3ba5ndg.jpeg)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73e61439cdc48836234deda711db6839ad975013",
        "id": "KzMOpJbaw32C"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "vgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "vgg19_model = VGG19(weights=vgg19_weights)\n",
        "_get_predictions(vgg19_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "864680ffb526762ccad64f0d6f95c886de12b2c3",
        "id": "W_NK0foUw32C"
      },
      "cell_type": "markdown",
      "source": [
        "## &nbsp;&nbsp; 1.3 InceptionNets\n",
        "\n",
        "Also known as GoogleNet consists of total 22 layers and was the winning model of 2014 image net challenge. \n",
        "\n",
        "- Inception modules are the fundamental block of InceptionNets. The key idea of inception module is to design good local network topology (network within a network)  \n",
        "- These modules or blocks acts as the multi-level feature extractor in which convolutions of different sizes are obtained to create a diversified feature map\n",
        "- The inception modules also consists of 1 x 1 convolution blocks whose role is to perform dimentionaltiy reduction.  \n",
        "- By performing the 1x1 convolution, the inception block preserves the spatial dimentions but reduces the depth. So the overall network's dimentions are not increased exponentially.  \n",
        "- Apart from the regular output layer, this network also consists of two auxillary classification outputs which are used to inject gradients at lower layers.  \n",
        "\n",
        "<br><br>\n",
        "\n",
        "The inception module is shown in the following figure:  \n",
        "\n",
        "![](https://hackathonprojects.files.wordpress.com/2016/09/inception_implement.png?w=649&h=337)\n",
        "\n",
        "The complete architecture is shown below: \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/2000/1*uXfC5fcbDsL0TJG4T8PsVw.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "### Pre-Trained Model : InceptionV3"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55f1b29e8180ddf39cf86886d2086566de21d0f4",
        "id": "ndpeEZ51w32D"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "inception_weights = '../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "inception_model = InceptionV3(weights=inception_weights)\n",
        "_get_predictions(inception_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bfb5488e2aaf9f46d92aed89021fc4a3a27bbf19",
        "id": "H45IiGTuw32D"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Resnets\n",
        "\n",
        "Original Paper : https://arxiv.org/pdf/1512.03385.pdf\n",
        "\n",
        "All the previous models used deep neural networks in which they stacked many convolution layers one after the other. It was learnt that deeper networks are performing better. However, it turned out that this is not really true. Following are the problems with deeper networks: \n",
        "\n",
        "- Network becomes difficult to optimize  \n",
        "- Vanishing / Exploding Gradeints  \n",
        "- Degradation Problem ( accuracy first saturates and then degrades )  \n",
        "\n",
        "### Skip Connections   \n",
        "\n",
        "So to address these problems, authors of the resnet architecture came up with the idea of skip connections with the hypothesis that the deeper layers should be able to learn something as equal as shallower layers. A possible solution is copying the activations from shallower layers and setting additional layers to identity mapping.   These connections are enabled by skip connections which are shown in the following figure. \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/987/1*pUyst_ciesOz_LUg0HocYg.png)\n",
        "\n",
        "So the role of these connections is to perform identity function over the activation of shallower layer, which in-turn produces the same activation. This output is then added with the activation of the next layer. To enable these connections or essentially enable this addition operation, one need to ensure the same dimentions of convolutions through out the network, that's why resnets have same 3 by 3 convolutions throughout. \n",
        "\n",
        "### Key Advantage \n",
        "\n",
        "By using residual blocks in the network, one can construct networks of any depth with the hypothesis that new layers are actually helping to learn new underlying patterns in the input data. The authors of the paper were able to create the deep neural network architecture with 152 layers. The variants of Resnets such as resnet34, resnet50, resnet101 have produced the solutions with very high accuracy in Imagenet competitions. \n",
        "\n",
        "### Why it works ? \n",
        "\n",
        "Lets discuss why residual networks are successful and enables the addition of more and more layers without the key problems ie. without hurting the network performance. \n",
        "\n",
        "Consider a plain neural network (A) without residual network as shown. So in the network (A) the input X is passed to this Neural Network (NN) to give the activation A1. \n",
        "\n",
        "  <br>\n",
        "\n",
        "![](https://i.imgur.com/9j8bKaY.png)\n",
        "\n",
        "Now, consider a more deeper network (B) in which a residual block (with 2 extra layers and a skip connection) is added in the previous network. So now, the activation A1 is being passed to Residual Block which in turns gives new activation A3. \n",
        "\n",
        "if there was no skip connection, then A3 was: \n",
        "\n",
        ">  A3 = relu ( W2 . A2 + b2)              ..... (without skip connection)\n",
        "\n",
        "where W2 and b2 are weights and bias associated with layer L2. But, with skip connection another term A1 will be passed to L2. So the equation of A3 will be modified as: \n",
        "\n",
        "> A3 = relu ( W2 . A2 + b2 + A1) \n",
        "\n",
        "If we use L2 regularization or the weight decay methods, they will force W2 and b2 to become close to zero. In the worst case, if these become zero, then \n",
        "\n",
        "> A3 = relu (A1)   \n",
        "\n",
        "because relu will output 0 for negative, A1 for positive and we know that A1 is previous activation from relu which is positive. \n",
        "\n",
        "> A3 = A1 \n",
        "\n",
        "This means that Identitiy function is easy for residual blocks to learn. By addition of residual blocks, model complexity was not increased. As this is only copying the previous activation to the next layers. However this is only the worst case situation, but the it may turn out that these additional layers learns something useful. In that case, the network performance will improve. \n",
        "\n",
        "Hence, adding the residual blocks / skip connections does not hurt the network performance but infact increases the chances that new layers will learn something useful. \n",
        "\n",
        "Let's look at the usage using pre-trained resnet 50 model. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e8441e52f70fe3103312dc9e1dd0cd103e90e66",
        "id": "h872r2sVw32D"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "resnet_weights = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "resnet_model = ResNet50(weights=resnet_weights)\n",
        "_get_predictions(resnet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53c26efe9ea54cfc0852ebe389fb05592307d864",
        "id": "p5-rtLmLw32D"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5 Xception Nets\n",
        "\n",
        "Xception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "584f18cd3ce168413f72076362db2663e6442baf",
        "id": "u_sKVd2fw32E"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.xception import Xception\n",
        "xception_weights = '../input/xception/xception_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "xception_model = Xception(weights=xception_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dadcad36efb34766ecb244797f6339e2ee14067f",
        "id": "AzDgN2rgw32E"
      },
      "cell_type": "markdown",
      "source": [
        "#### Comparison of different architectures: \n",
        "\n",
        "The following image describes the relative comparison of these architectures in terms of performance as size. \n",
        "<br><br>\n",
        "\n",
        "![](http://www.houseofbots.com/images/news/573/cover.png)\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## 2. Image Feature Extraction using PreTrained Models \n",
        "\n",
        "Lets look at how one can use pre-trained models for feature extraction, The extracted features can be used for Machine Learning purposes. \n",
        "\n",
        "First step is to load the weights of the pre-trained model in the model architecture. Notice, that an additional argument is passed include_top = False, which states that we do not want to add the last layer of this architecture. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fd638c0b5f124f0dc5805ca31a8517d6df1f6d4",
        "id": "YcB743ksw32E"
      },
      "cell_type": "code",
      "source": [
        "resnet50 = ResNet50(weights='imagenet', include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf05582ea7eecc9e839f996d218ad2c98ce3c768",
        "id": "JMOKsYa3w32E"
      },
      "cell_type": "markdown",
      "source": [
        "As the next step, we will pass an image to this model and identify the features. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6eaca5c0a04641da7a0e592f5b41d3bd19139989",
        "id": "pDOZYAKqw32F"
      },
      "cell_type": "code",
      "source": [
        "def _get_features(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_data = image.img_to_array(img)\n",
        "    img_data = np.expand_dims(img_data, axis=0)\n",
        "    img_data = preprocess_input(img_data)\n",
        "    resnet_features = resnet50.predict(img_data)\n",
        "    return resnet_features\n",
        "\n",
        "img_path = \"../input/dogs-vs-cats-redux-kernels-edition/train/dog.2811.jpg\"\n",
        "resnet_features = _get_features(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ebd7b1dd8ad79987ca62bc6cc712c3538a9ee21e",
        "id": "gWzbHB9-w32F"
      },
      "cell_type": "markdown",
      "source": [
        "now the extracted features are stored in the variable resnet_features. One can flatten them or sequee them in order to use them in ML models.  Flatten will produce a long vector of feature elements. Squeeze will produce a 3D matrix of the features"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb48827bfad5ff9bfddbe9df991d32e3449f9451",
        "id": "bNMx_OAhw32F"
      },
      "cell_type": "code",
      "source": [
        "features_representation_1 = resnet_features.flatten()\n",
        "features_representation_2 = resnet_features.squeeze()\n",
        "\n",
        "print (\"Shape 1: \", features_representation_1.shape)\n",
        "print (\"Shape 2: \", features_representation_2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "145461de10bdd86deeaa1bfb27af0a4e5f8eaeff",
        "id": "dWXg4Hkgw32F"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Transfer Learning Example \n",
        "\n",
        "<br>\n",
        "\n",
        "Lets look at the implemetation of transfer learning using pre-trained model features. First, we 'll create a dataset containing two classes of images : bananas and strawberrys. Also add a test dataset contianing images from both classes.\n",
        "\n",
        "### 3.1 Dataset Preparation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d075c7be0c290ff513df855fe1707ea145ab9330",
        "id": "IIbGsqHww32F"
      },
      "cell_type": "code",
      "source": [
        "basepath = \"../input/fruits/fruits-360_dataset/fruits-360/Training/\"\n",
        "class1 = os.listdir(basepath + \"Banana/\")\n",
        "class2 = os.listdir(basepath + \"Strawberry/\")\n",
        "\n",
        "data = {'banana': class1[:10], \n",
        "        'strawberry': class2[:10], \n",
        "        'test': [class1[11], class2[11]]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c3f81bf1bdf0266e09e180ef4fecd8c4e61cc8e",
        "id": "UIWoEhagw32G"
      },
      "cell_type": "markdown",
      "source": [
        "Transfer learning can be implemented in two steps: \n",
        "\n",
        "Step 1 : Image Feature Exraction  \n",
        "Step 2 : Training a Classifier  \n",
        "\n",
        "### Step 1 : Feature Extraction using pre-trained models (resnet50)\n",
        "\n",
        "Iterate in the images, call the same function used in point 2 for image feature extraction, we will use the flatten representation of these features "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "715ae76b5aca0b8fce478fe1a3c2c9639c7bd789",
        "id": "PTmMJ21Tw32G"
      },
      "cell_type": "code",
      "source": [
        "features = {\"banana\" : [], \"strawberry\" : [], \"test\" : []}\n",
        "testimgs = []\n",
        "for label, val in data.items():\n",
        "    for k, each in enumerate(val):        \n",
        "        if label == \"test\" and k == 0:\n",
        "            img_path = basepath + \"/Banana/\" + each\n",
        "            testimgs.append(img_path)\n",
        "        elif label == \"test\" and k == 1:\n",
        "            img_path = basepath + \"/Strawberry/\" + each\n",
        "            testimgs.append(img_path)\n",
        "        else: \n",
        "            img_path = basepath + label.title() + \"/\" + each\n",
        "        feats = _get_features(img_path)\n",
        "        features[label].append(feats.flatten())        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fecb0e6eb0759b69ccf58cb255cc2f6751216b3",
        "id": "Cah5BtK2w32G"
      },
      "cell_type": "markdown",
      "source": [
        "Next, Convert the features from dictionary format to pandas dataframe. A long dataframe will be created. I will be applying variance filter later on this dataframe to reduce the dimentionality. Other ideas to avoid this step : perform PCA / SVD to obtain the dense features. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "57970a5ba3a19144849babd51102424a531cd207",
        "id": "l8m2k5NGw32G"
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame()\n",
        "for label, feats in features.items():\n",
        "    temp_df = pd.DataFrame(feats)\n",
        "    temp_df['label'] = label\n",
        "    dataset = dataset.append(temp_df, ignore_index=True)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "254f97b6e4cecd5a8b8e4ee3ddf300a4d878968e",
        "id": "OzLt2KU1w32G"
      },
      "cell_type": "markdown",
      "source": [
        "Prepare X (predictors) and y (target) from the dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8732b6bd71eb01c4fc7bda6faf7d999d062aab76",
        "id": "gGHVZilEw32G"
      },
      "cell_type": "code",
      "source": [
        "y = dataset[dataset.label != 'test'].label\n",
        "X = dataset[dataset.label != 'test'].drop('label', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9af01bdf68adfce750b88fa13ff4cd85db580b4",
        "id": "dUcdpN8Jw32H"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: Write a classifier to predict two classes\n",
        "\n",
        "we will write a simple neural network (multi layer perceptron classifier) using sklearn for training purposes. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d65f6ef1b025b1913bcfb41b091280cd2cb68f33",
        "id": "ros4qIkRw32H"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(100, 10))\n",
        "pipeline = Pipeline([('low_variance_filter', VarianceThreshold()), ('model', model)])\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "print (\"Model Trained on pre-trained features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8408e267c963ab8c3b49564a1ef833c4ec87b17a",
        "id": "dv2iM9p_w32H"
      },
      "cell_type": "markdown",
      "source": [
        "Let's predict the output on new images and check the outcome."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "feeefbaa75207a3c4bea5dc189b47dd7269224ff",
        "id": "AA-Ci_Zkw32H"
      },
      "cell_type": "code",
      "source": [
        "preds = pipeline.predict(features['test'])\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "for i in range(2):\n",
        "    ax[i].imshow(Image.open(testimgs[i]).resize((200, 200), Image.ANTIALIAS))\n",
        "    ax[i].text(10, 180, 'Predicted: %s' % preds[i], color='k', backgroundcolor='red', alpha=0.8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd8247fb898e1666eaf4e76026a6b332ef686970",
        "id": "XTZUp7NDw32H"
      },
      "cell_type": "markdown",
      "source": [
        "So a simple neural network with only 20 rows of training data is able to correctly classify the two images on test set. \n",
        "\n",
        "### EndNotes \n",
        "Thanks for viewing this kernel, If you liked it, please upvote. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "cnn-architectures-vgg-resnet-inception-tl.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}