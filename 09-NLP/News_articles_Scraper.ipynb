{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " News_articles_Scraper.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/Personnel/blob/master/News_articles_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwcSbg1Rnhrh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "***Uncomment the line to install newspaper3k first***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhMyK6-bRrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "930df9e3-e082-4500-8abf-52474792ab77"
      },
      "source": [
        "! pip install newspaper3k"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |‚ñà‚ñå                              | 10kB 20.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà                             | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñã                           | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                        | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 81kB 8.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/cf/d0ff82625e53bd245d6173ce6333d190abbfcd94e4c30e54b4e16b474216/tldextract-2.2.3-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 7.2MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4MB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/84/df6de99cba01afc82344c9cb3a79df100a00ac33396120f8aa66c72f0d84/feedparser-6.0.1-py2.py3-none-any.whl (80kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: feedfinder2, jieba3k, tinysegmenter, sgmllib3k\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=a602f167aba632a4ac2e6d1d00da6bc996fda12d035266b50daccc7b4eea0ca4\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=8f094d30b06560ef52324f831fd7911ce3c6eeb66028f166308467a768c81d2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=e92c2a8552e4e3a1c7dd6e43375994579aeadfca4c70b278d8ac8a818785e4d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=624b86b3444338574e1b4285b46831558d2fae2450e7b031bce961ef47c23579\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built feedfinder2 jieba3k tinysegmenter sgmllib3k\n",
            "Installing collected packages: feedfinder2, cssselect, requests-file, tldextract, jieba3k, tinysegmenter, sgmllib3k, feedparser, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-2.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwdPRAmkOvNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "import pickle\n",
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AktNJdfUPlZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting links for all the pages (1 to 158) of boomlive fake news section\n",
        "fakearticle_links = []\n",
        "for i in range(1, 159):\n",
        "  url = 'https://www.boomlive.in/fake-news/' + str(i)\n",
        "  try:\n",
        "    # this might throw an exception if something goes wrong.\n",
        "    page=requests.get(url) \n",
        "\n",
        "    # send requests\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')  \n",
        "\n",
        "    # Collecting all the links in a list\n",
        "    for content in soup.find_all('h2', attrs={'class':'entry-title'}):\n",
        "      link = content.find('a')\n",
        "      fakearticle_links.append(link.get('href')) \n",
        "  \n",
        "  # this describes what to do if an exception is thrown \n",
        "  except Exception as e:    \n",
        "    # get the exception information\n",
        "    error_type, error_obj, error_info = sys.exc_info()      \n",
        "    #print the link that cause the problem\n",
        "    print ('ERROR FOR LINK:',url)\n",
        "    #print error info and line that threw the exception                          \n",
        "    print (error_type, 'Line:', error_info.tb_lineno)\n",
        "    continue\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RySM08K5ICs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "813fb602-4a91-46dc-9d03-e8a5638751d3"
      },
      "source": [
        "fakearticle_links[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/fake-news/image-of-bus-with-br-ambedkar-and-his-wifes-photo-across-it-is-fake-9829',\n",
              " '/fake-news/2018-images-from-protest-in-lucknow-revived-with-unemployment-twist-9827',\n",
              " '/fake-news/photos-of-nayva-naveli-nanda-partying-falsely-shared-as-shweta-bachchan-9814',\n",
              " '/fake-news/no-amitabh-bachchan-is-not-posing-with-dawood-ibrahim-in-the-photo-9813',\n",
              " '/fake-news/forced-to-vote-for-the-shiv-sena-kangana-ranauts-claim-is-false-9805']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oexODsG5STe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f395f68-d2ae-40ed-e6ea-8cb4c37175d5"
      },
      "source": [
        "len(fakearticle_links)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1896"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5GbV7F7gx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "caed2427-1f18-46a5-c11c-36ef4f6f2e6d"
      },
      "source": [
        "fakearticle_links[1888:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/fake-news-police-no-5-day-week-for-banks-on-cards/',\n",
              " '/fake-news-police-whatsapp-message-islamic-state-group-fake/',\n",
              " '/fake-news-police-twitter-video-of-jet-airways-aircraft-on-fire-is-fake/',\n",
              " '/google-launches-fact-check-feature-globally-to-fight-fake-news/',\n",
              " '/fake-news-or-real-news-media-with-an-agenda/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P91gj-eKLVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8RB1V7xwA2X",
        "colab_type": "text"
      },
      "source": [
        "We have to modify the links so that the links actually work as we can see that the string extracted is the last part of the url!\n",
        "\n",
        "**We have to add 'https://www.boomlive.in/fake-news' to the extracted links.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBigi0fCv-uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modify the links so that it takes us to the particular website\n",
        "str1 = 'https://www.boomlive.in/fake-news'\n",
        "fakearticle_links = [str1+lnk for lnk in fakearticle_links]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sco6gAsix3tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a0f6fb9-a2d7-4cc2-be52-f3aaf83c617f"
      },
      "source": [
        "fakearticle_links[6:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.boomlive.in/fake-news/fake-news/toi-ht-abp-news-fall-for-fake-account-impersonating-indrajit-chakraborty-9671',\n",
              " 'https://www.boomlive.in/fake-news/fake-news/did-sushant-singh-rajput-conceptualise-upcoming-mobile-game-fau-g-9666',\n",
              " 'https://www.boomlive.in/fake-news/fake-news/no-amartya-sen-did-not-say-pubg-ban-to-affect-indias-economy-9658',\n",
              " 'https://www.boomlive.in/fake-news/fake-news/video-of-bengaluru-woman-abusing-2-year-old-grandson-viral-as-mumbai-9657']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xucBh7fSyBIF",
        "colab_type": "text"
      },
      "source": [
        "**The links are modified and is working :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QnolFp-yKlp",
        "colab_type": "text"
      },
      "source": [
        "***Creating a dataset of all the fake articles***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZrN-fWPygUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataset for storing the news articles\n",
        "news_dataset = pd.DataFrame(fakearticle_links, columns=['URL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "181ydS6G3BdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "269b4cab-8567-4fb5-c85f-2fc995c1e5a8"
      },
      "source": [
        "news_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/20...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 URL\n",
              "0  https://www.boomlive.in/fake-news/fake-news/cr...\n",
              "1  https://www.boomlive.in/fake-news/fake-news/un...\n",
              "2  https://www.boomlive.in/fake-news/fake-news/cr...\n",
              "3  https://www.boomlive.in/fake-news/fake-news/no...\n",
              "4  https://www.boomlive.in/fake-news/fake-news/20..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51RJc1Oy2WbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title, text, summary, keywords, published_on, author = [], [], [], [], [], []   # Creating empty lists to store the data\n",
        "for Url in fakearticle_links:\n",
        "  article = Article(Url)\n",
        "\n",
        "  #Call the download and parse methods to download information\n",
        "  try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  # Scrape the contents of article\n",
        "  title.append(article.title)                    # extracts the title of the article\n",
        "  text.append(article.text)                      # extracts the whole text of article\n",
        "  summary.append(article.summary)                # gives us a summary abou the article\n",
        "  keywords.append(', '.join(article.keywords))   # the main keywords used in it\n",
        "  published_on.append(article.publish_date)      # the date on which it was published\n",
        "  author.append(article.authors)                 # the authors of the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F76iH9zah8Bo",
        "colab_type": "text"
      },
      "source": [
        "**Checking the lists created**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B2cjmJ1Btn1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "5fe013e0-6e58-47bf-9f99-23ad207a33c7"
      },
      "source": [
        "text[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Several mainstream news outlets such as the Times of India, Hindustan Times and ABP News fell for a fake Twitter account impersonating actress Rhea Chakraborty\\'s father Indrajit Chakraborty to falsely claim that the latter was tweeting following his daughter\\'s arrest.\\n\\nThe Narcotics Control Bureau on Tuesday arrested Rhea Chakraborty for allegedly procuring drugs for late actor Sushant Singh Rajput. The NCB is investigating the drug links in Rajput\\'s death. The actor was found dead in his apartment on June 14, 2020 in Mumbai and the case has since been transferred to the Central Bureau of Investigation.\\n\\nNews outlets such as the Times of India, Hindustan Times and ABP News cited tweets from an impostor account pretending to be Indrajit Chakraborty.\\n\\n\\n\\n\\n\\nBelow is a screenshot of the Hindustan Times article.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe impostor account in a series of tweets blamed the judiciary and the media for the treatment meted out to his children post the death of Sushant Singh Rajput.\\n\\nThe fake account is neither verified nor does it contain a disclaimer in its bio that states it is a fan account.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOne such tweet by the fake account states how Indrajit Chakraborty has been shattered after the arrest and contemplated ending his life. The text of the tweet reads, \"No father can bear injustice on his daughter. I should die #JusticeForRhea.\" (Here is the archive of the tweet).\\n\\nAnother tweet, where the impostor account is questioning Rhea Chakraborty\\'s trial by media was also cited in the articles. The tweet reads, \"The entire country is determined to send Rhea to the gallows despite no proof.\" (Original text in Hindi: ‡§¨‡§ó‡•à‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡§¨‡•Ç‡§§ ‡§ï‡•á ‡§™‡•Ç‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§∞‡§ø‡§Ø‡§æ ‡§ï‡•ã ‡§´‡§æ‡§Ç‡§∏‡•Ä ‡§™‡§∞ ‡§≤‡§ü‡§ï‡§æ‡§®‡•á ‡§ï‡•ã ‡§§‡•Å‡§≤‡§æ ‡§π‡•à#JusticeforRhea).\\n\\nThe ABP news article based on tweets of the fake account is headlined as \"Rhea Chakraborty\\'s Father Indrajit On Her Arrest: \\'All This Because Her Now Dead Boyfriend Smoked Weed?\" ABP News published two articles based on the impostor account\\'s tweets.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe second article is headlined as, \"Rhea Chakraborty\\'s Father Indrajit Says \\'I Should Die, No Father Can Bear Injustice On His Daughter\"\\n\\nOther websites that fell for the fake account include Sakal Times, Hindi daily Amar Ujala and entertainment website Pinkvilla.\\n\\n\\n\\nClick here for Sakal Times\\' archive, here for Hindustan Times archive, here for Times of India archive, here for Amar Ujala\\'s archive, Pinkvilla\\'s archive, here for India TV News\\' archive and Lokmat English\\'s archive. Click here for Zee News\\' archive.\\n\\nAlso Read:Did Sushant Singh Rajput Conceptualise Upcoming Mobile Game Fau-G?\\n\\n\\n\\nFact Check\\n\\n\\n\\nBOOM was able to ascertain that the account is an impostor one and has impersonated Chakraborty\\'s father. We scoured through old tweets and replies to the handle.\\n\\n\\n\\nWe found a reply to the handle from September 6, which showed that the user earlier had the username @WeWantRahuI (spelt with a capital I).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here to view an archive of the tweet which indicates the same.\\n\\nThe same account, with the previous username @WeWantRahuI tweeted on June 8 urging netizens to increase his followers to 1500. A screenshot of the previous account was also posted with the tweet. According to the screenshot posted, the bio of the account reads, \"‡§ï‡§π‡•ã ‡§¶‡§ø‡§≤ ‡§∏‡•á ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§´‡§ø‡§∞ ‡§∏‡•á, ‡§∞‡§æ‡§π‡•Å‡§≤ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§´‡•â‡§∞ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§∞‡§æ‡§π‡•Å‡§≤ ‡§ó‡§æ‡§Ç‡§ß‡•Ä ‡§ú‡•Ä ‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§ï‡§æ ‡§Ö‡§ó‡§≤‡§æ PM ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡•á ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏ ‡§™‡•á‡§ú ‡§ï‡•ã ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡•á‡§Ç‡•§\" (Say from your heart, Congress again. Rahul Gandhi from prime minister. To make Rahul Gandhi ji the next PM and to support him more and more, follow this page)\\n\\nThe account, which was opened in December last year, has around 7000 followers. The edited bio now reads, \"Satyameva Jayate.\"\\n\\n\\n\\n\\n\\n8 More to go.... pic.twitter.com/3cFpI3V1XN ‚Äî Indrajit Chakraborty (@IndrajitChakra) June 8, 2020\\n\\n‡§∞‡§æ‡§π‡•Å‡§≤ ‡§ú‡•Ä ‡§ï‡•Ä ‡§∏‡•ã‡§ö ‡§ï‡•ã ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§§‡•á ‡§π‡•à ‡§π‡§Æ‡•§\\n\\n\\n\\n‡§π‡§Æ ‡§∏‡§¨ ‡§π‡•à ‡§≠‡§æ‡§à ‡§≠‡§æ‡§à‡•§\\n\\n\\n\\n‡§∏‡§≠‡•Ä ‡§∏‡§æ‡§•‡•Ä ‡§è‡§ï ‡§¶‡•Ç‡§∏‡§∞‡•á ‡§ï‡•ã ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡•á‡§Ç, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§ï‡•ã‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§π‡§æ‡§• ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§π‡•ãüôè ‚Äî Indrajit Chakraborty (@IndrajitChakra) September 4, 2020\\n\\nBOOM found that the account\\'s previous tweets are mostly pro Congress and pro Rahul Gandhi.\\n\\n\\n\\n\\n\\n\\n\\nBOOM has also reached to to Chakraborty\\'s lawyer, Satish Manshinde to know if Indrajit Chakraborty is on any social media platforms. The article will be updated if we get response.\\n\\nPost our fact check, publishers including The Times of India, Zee News, Hindustan Times, Live Hindustan, Sakal, Amar Ujala, Pinkvilla, ABP News, Maharashtra Times have either deleted the story or issued corrections'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmFrYPz1gJl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "474be8e5-2d78-4403-a6ef-263799e316be"
      },
      "source": [
        "keywords[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'jammu, video, sena, image, archive, post, kangana, unrelated, support, ranaut, singh, mumbai, cars, karni, viral, senas, visuals'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZDEu1_CgPny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b80f069a-9713-4637-83d2-922b6d615aa1"
      },
      "source": [
        "published_on[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2020, 9, 9, 13, 24, 20, tzinfo=tzoffset(None, 19800))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCV5MJLngkwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2434541b-42ff-42f8-f8c1-1ad224c1b637"
      },
      "source": [
        "author[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Swasti Chatterjee']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBUYn0XbhCPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the columns in the fake news dataset\n",
        "news_dataset['title'] = title\n",
        "news_dataset['text'] = text\n",
        "news_dataset['keywords'] = keywords\n",
        "news_dataset['published date'] = published_on\n",
        "news_dataset['author'] = author"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUrLIY-0g9g3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b6997f08-0487-4cf7-b69d-de7116943354"
      },
      "source": [
        "# Check the first five columns of dataset created\n",
        "news_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>published date</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/cr...</td>\n",
              "      <td>Cropped Image Of Asansol Municipality Signboar...</td>\n",
              "      <td>A cropped image of a signboard of Asansol Muni...</td>\n",
              "      <td>false, image, claims, written, text, hindi, as...</td>\n",
              "      <td>2020-09-11 14:55:25+05:30</td>\n",
              "      <td>[Sk Badiruddin]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/un...</td>\n",
              "      <td>Unrelated Visuals Viral As Karni Sena's Show O...</td>\n",
              "      <td>Old unrelated images from different states inc...</td>\n",
              "      <td>jammu, video, sena, image, archive, post, kang...</td>\n",
              "      <td>2020-09-11 14:37:28+05:30</td>\n",
              "      <td>[Dilip Unnikrishnan]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/cr...</td>\n",
              "      <td>Cropped Video Of A Multi-Faith Ceremony For Ra...</td>\n",
              "      <td>A video of a priest reading a verse from the B...</td>\n",
              "      <td>read, jets, misleading, video, religious, shar...</td>\n",
              "      <td>2020-09-11 14:16:32+05:30</td>\n",
              "      <td>[Anmol Alphonso]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/no...</td>\n",
              "      <td>No, This Is Not A BJP Leader Threatening Judge...</td>\n",
              "      <td>A video from a 2015 episode of a dance reality...</td>\n",
              "      <td>bjp, episode, video, father, threatening, vira...</td>\n",
              "      <td>2020-09-10 21:29:18+05:30</td>\n",
              "      <td>[Anmol Alphonso]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news/20...</td>\n",
              "      <td>2017 Video Of Teen Assaulted By A Man In Andhr...</td>\n",
              "      <td>A 2017 video of a man groping and assaulting a...</td>\n",
              "      <td>hindus, ‡¶π‡¶®‡¶¶, 2017, video, peddled, kerala, man...</td>\n",
              "      <td>2020-09-10 20:03:03+05:30</td>\n",
              "      <td>[Suhash Bhattacharya]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 URL  ...                 author\n",
              "0  https://www.boomlive.in/fake-news/fake-news/cr...  ...        [Sk Badiruddin]\n",
              "1  https://www.boomlive.in/fake-news/fake-news/un...  ...   [Dilip Unnikrishnan]\n",
              "2  https://www.boomlive.in/fake-news/fake-news/cr...  ...       [Anmol Alphonso]\n",
              "3  https://www.boomlive.in/fake-news/fake-news/no...  ...       [Anmol Alphonso]\n",
              "4  https://www.boomlive.in/fake-news/fake-news/20...  ...  [Suhash Bhattacharya]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuLdaPfTigcb",
        "colab_type": "text"
      },
      "source": [
        "**Converting the dataset to a csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBB9t8hwifOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_dataset.to_csv('Fake_news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6rTLF74jEVf",
        "colab_type": "text"
      },
      "source": [
        "**Reading the csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwD-yE21jCwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('Fake_news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNrmP-Z5jPWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b2ff9ac0-6fef-42f3-a1b5-f4224cedf23e"
      },
      "source": [
        "# Checking the last 5 rows of the csv file\n",
        "df.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>URL</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>keywords</th>\n",
              "      <th>published date</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1888</th>\n",
              "      <td>1888</td>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news-po...</td>\n",
              "      <td>Fake News Police: No 5-Day Week For Banks On C...</td>\n",
              "      <td>People queue to deposit inside a bank in Allah...</td>\n",
              "      <td>notification, week, fake, 5day, bank, message,...</td>\n",
              "      <td>2017-04-13 16:24:42+05:30</td>\n",
              "      <td>['Jency Jacob']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>1889</td>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news-po...</td>\n",
              "      <td>Fake News Police: 'Firdaus We Ascend' Islamic ...</td>\n",
              "      <td>A WhatsApp message warning people about joinin...</td>\n",
              "      <td>fake, read, report, user, group, message, user...</td>\n",
              "      <td>2017-04-12 19:52:45+05:30</td>\n",
              "      <td>['A Staff Writer']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1890</th>\n",
              "      <td>1890</td>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news-po...</td>\n",
              "      <td>Fake News Police: Twitter Video Of Jet Airways...</td>\n",
              "      <td>Screenshot of a Twitter user page\\n\\nThis vide...</td>\n",
              "      <td>fake, twitter, video, airways, jet, aircraft, ...</td>\n",
              "      <td>2017-04-12 16:22:58+05:30</td>\n",
              "      <td>['A Staff Writer']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>1891</td>\n",
              "      <td>https://www.boomlive.in/fake-news/google-launc...</td>\n",
              "      <td>Google Takes On Fake News With New Fact Check ...</td>\n",
              "      <td>Tech major Google is the latest to tackle the ...</td>\n",
              "      <td>fake, search, company, google, claim, users, i...</td>\n",
              "      <td>2017-04-08 11:16:09+05:30</td>\n",
              "      <td>['A Staff Writer']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>1892</td>\n",
              "      <td>https://www.boomlive.in/fake-news/fake-news-or...</td>\n",
              "      <td>Fake News Or Real News? Media With An Agenda</td>\n",
              "      <td>There is a sizeable shift in perception going ...</td>\n",
              "      <td>fake, agenda, london, south, issue, world, rea...</td>\n",
              "      <td>2017-04-03 18:25:44+05:30</td>\n",
              "      <td>['Siddhartha Dubey']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                author\n",
              "1888        1888  ...       ['Jency Jacob']\n",
              "1889        1889  ...    ['A Staff Writer']\n",
              "1890        1890  ...    ['A Staff Writer']\n",
              "1891        1891  ...    ['A Staff Writer']\n",
              "1892        1892  ...  ['Siddhartha Dubey']\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv4dvhKWkPKO",
        "colab_type": "text"
      },
      "source": [
        "**Download the csv file in local machine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvwmrMbZjcvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "23a8f430-8e02-40cd-f6a8-5b7f42f2a2b6"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Fake_news.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b0772046-511a-4294-abe2-0b88c447572b\", \"Fake_news.csv\", 6372482)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLkxEvJsanhl",
        "colab_type": "text"
      },
      "source": [
        "**Scraping news from Times of India**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo4pevVSemOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOIarticle_links = []  # Creating an empty list of all the urls of news from Times of India site"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF7_TOdeamwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting links for all the pages (2 to 125) of boomlive fake news section\n",
        "for i in range(2, 126):\n",
        "  url = 'https://timesofindia.indiatimes.com/news/' + str(i)\n",
        "  \n",
        "  try:\n",
        "    # send requests\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')  \n",
        "\n",
        "    # Collecting all the links in a list\n",
        "    for content in soup.find_all('span', attrs={'class':'w_tle'}):\n",
        "      link = content.find('a')\n",
        "      TOIarticle_links.append(link.get('href')) \n",
        "  \n",
        "  # this describes what to do if an exception is thrown \n",
        "  except Exception as e:    \n",
        "    # get the exception information\n",
        "    error_type, error_obj, error_info = sys.exc_info()      \n",
        "    #print the link that cause the problem\n",
        "    print ('ERROR FOR LINK:',url)\n",
        "    #print error info and line that threw the exception                          \n",
        "    print (error_type, 'Line:', error_info.tb_lineno)\n",
        "    continue\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sax-X1RnuSZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b2c674dd-bdea-4628-ecde-4dd979c6b6da"
      },
      "source": [
        "TOIarticle_links[6:15]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-93bcdca42c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTOIarticle_links\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TOIarticle_links' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SDBCT1qbdRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(TOIarticle_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4fytA09uae9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str2 = 'https://timesofindia.indiatimes.com'\n",
        "TOIarticle_links = [str2+lnk for lnk in TOIarticle_links if lnk[0]=='/']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUrPKcTxwTUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOIarticle_links[5:8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsm1srPOwft9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(TOIarticle_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cyifXyNvqiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title, text, summary, keywords, published_on, author = [], [], [], [], [], []   # Creating empty lists to store the data\n",
        "for Url in TOIarticle_links:\n",
        "  article = Article(Url)\n",
        "\n",
        "  #Call the download and parse methods to download information\n",
        "  try:\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "  # Scrape the contents of article\n",
        "  title.append(article.title)                    # extracts the title of the article\n",
        "  text.append(article.text)                      # extracts the whole text of article\n",
        "  summary.append(article.summary)                # gives us a summary abou the article\n",
        "  keywords.append(', '.join(article.keywords))   # the main keywords used in it\n",
        "  published_on.append(article.publish_date)      # the date on which it was published\n",
        "  author.append(article.authors)                 # the authors of the article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg7k3w4CLhcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFwkKJKNLwrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOI_dataset = pd.DataFrame(TOIarticle_links, columns=['URL'])\n",
        "# Adding the columns in the TOI news dataset\n",
        "TOI_dataset['title'] = title\n",
        "TOI_dataset['text'] = text\n",
        "TOI_dataset['keywords'] = keywords\n",
        "TOI_dataset['published date'] = published_on\n",
        "TOI_dataset['author'] = author"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3gkNLxgONBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOI_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Im2drfGOjpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOI_dataset.to_csv('TOI_news_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efx-qUWeOvQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt = pd.read_csv('TOI_news_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovt81Pm1O4fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt.tail(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1jmV5WfO78Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('TOI_news_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}