{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spacy\n",
    "\n",
    "**spaCy** is an open-source Python library that parses and \"understands\" large volumes of text. \n",
    "\n",
    "* spaCy is the best way to prepare text for deep learning.\n",
    "* It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem.\n",
    "* With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems. \n",
    "\n",
    "* spaCy excels at large-scale information extraction tasks. \n",
    "* It's written from the ground up in carefully memory-managed Cython. \n",
    "* Independent research in 2015 found spaCy to be the fastest in the world. \n",
    "* If your application needs to process entire web dumps, spaCy is the library you want to be using. \n",
    "\n",
    "* spaCy is designed to help you do real work — to build real products, or gather real insights. \n",
    "* The library respects your time, and tries to avoid wasting it. \n",
    "* It's easy to install, and its API is simple and productive. \n",
    "\n",
    "Reference and for more details, refer (https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "***Create the virtual environment first***\n",
    "\n",
    "> `conda create -n spacyenv python=3`\n",
    "\n",
    "***See the list of env available including the newly created spacyenv***\n",
    "> `conda info -e`\n",
    "\n",
    "***Activate the env***\n",
    "> `conda activate spacyenv`\n",
    "\n",
    "***Jupyter Notebook makes sure that the IPython kernel is available, but you have to manually add a kernel with a different version of Python or a virtual environment. First, you need to activate your virtual environment. Next, install ipykernel which provides the IPython kernel for Jupyter:***\n",
    "> `pip install --user ipykernel`\n",
    "\n",
    "***Now, add your virtual environment to Jupyter***\n",
    "> `python -m ipykernel install --user --name=spacyenv`\n",
    "\n",
    "Now your env is created and added to jupyter notebook.\n",
    "Now to Install Spacy we have two options\n",
    "\n",
    "### 1. From the command line or terminal:\n",
    "> `conda install -c conda-forge spacy`\n",
    "> <br>*or*<br>\n",
    "> `pip install -U spacy`\n",
    "\n",
    "\n",
    "### 2. From Jupyter Notebook\n",
    "\n",
    "***Start the Jupyter Notebook from command line by typing***\n",
    "> `jupyter notebook`\n",
    "\n",
    "***Now Install Spacy by typing and executing***\n",
    "> `!pip install -U spacy`\n",
    "\n",
    "\n",
    "### Now download the language specific model. I will download the model for english langauge as below.\n",
    "#### (you must run this as admin or use sudo)\n",
    "\n",
    "***From command line***\n",
    "> `python -m spacy download en`\n",
    "\n",
    "***from jupyter notebook***\n",
    "> `!python -m spacy download en`\n",
    "\n",
    "> ### If successful, you should see a message like:\n",
    "\n",
    "> **`Linking successful`**<br>\n",
    "> `    C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\en_core_web_sm -->`<br>\n",
    "> `    C:\\Anaconda3\\envs\\spacyenv\\lib\\site-packages\\spacy\\data\\en`<br>\n",
    "> ` `<br>\n",
    "> `    You can now load the model via spacy.load('en')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic operations to check if everything installed correctly\n",
    "\n",
    "* This is a typical set of instructions for importing and working with spaCy. \n",
    "* Don't worry, if it takes time - spaCy has large library to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona PROPN nsubj\n",
      "will VERB aux\n",
      "go VERB ROOT\n",
      "very ADV advmod\n",
      "soon ADV advmod\n",
      ". PUNCT punct\n",
      "Do VERB aux\n",
      "not ADV neg\n",
      "get VERB ROOT\n",
      "panic NOUN dobj\n",
      ". PUNCT punct\n",
      "Cases NOUN nsubj\n",
      "in ADP prep\n",
      "U.S. PROPN pobj\n",
      "have VERB aux\n",
      "reduced VERB ROOT\n",
      "in ADP prep\n",
      "last ADJ amod\n",
      "48 NUM nummod\n",
      "hours NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(u'Corona will go very soon. Do not get panic. Cases in U.S. have reduced in last 48 hours')\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply some formating on how the output is printed. However for now observe few things as below:\n",
    "1. Corona  is recognized as a Proper Noun, not just a word at the start of a sentence\n",
    "2. U.S. is kept together as one entity (It is called 'token')\n",
    "\n",
    "Later we will see what each of these abbreviations mean and how they're derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.   Image source: https://spacy.io/usage/spacy-101#pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pipeline1.png\" width = \"600\" height = \"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see what operations are inside the pipeline using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.Tagger at 0x13c507d9ec8>),\n",
       " ('parser', <spacy.pipeline.DependencyParser at 0x13c507da588>),\n",
       " ('ner', <spacy.pipeline.EntityRecognizer at 0x13c507dab28>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "* The first step in processing text is to split up all the component parts (words & punctuation) into \"tokens\". \n",
    "* These tokens are annotated inside the Doc object to contain descriptive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is VERB aux\n",
      "n't ADV neg\n",
      "looking VERB ROOT\n",
      "  SPACE \n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"Apple isn't looking  at buying U.K. startup.\")\n",
    "for token in doc3:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice how `isn't` has been split into two tokens. \n",
    "* spaCy recognizes both the root verb `is` and the negation attached to it. \n",
    "* Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc3 also contains the original text. You can see it by executing the `doc3` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple isn't looking  at buying U.K. startup."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "print(doc3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging (POS)\n",
    "* The next step after splitting the text up into tokens is to assign parts of speech. \n",
    "* In the above example, `Apple` was recognized to be a ***proper noun***. Here some statistical modeling is required. \n",
    "* For example, words that follow \"the\" are typically nouns.\n",
    "\n",
    "For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is VERB\n",
      "n't ADV\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "\n",
    "for token in doc4:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full name of a tag use `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Important information which Spacy assign to tokens.\n",
    "\n",
    "Will cover all these in detail in next articles under NLP Spacy Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Tag|Description|doc4[0].tag|\n",
    "|:------|:------:|:------|\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Apple`|\n",
    "|`.lemma_`|The base form of the word|`Apple`|\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span (Slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "at buying U.K."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "sliced_text = doc5[4:7]\n",
    "sliced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sliced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "\n",
    "Print sentence tokens instead of word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
    "for sent in doc6.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc6[6].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc6[11].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6[2].is_sent_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note: \n",
    "    This article is \"Article 1\" under the NLP Spacy Series of articles\n",
    "    Here, I have covered the installation of Spacy and its basic operations.\n",
    "    Next article, I will be covering the text preprocessing steps in detail using Spacy.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacyenv",
   "language": "python",
   "name": "spacyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
