{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeP2K1rid4mkT93IoeMO3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/DataScience-Enterprise-Architect/blob/master/Data%20Science%20Coding%20Interview/Implement_Gradient_Descent_Variants_with_MSE_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Gradient Descent Variants with MSE Loss\n",
        "In this problem, you need to implement a single function that can perform three variants of gradient descent—Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent—using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n",
        "\n",
        "Example\n",
        "\n",
        "Example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
        "y = np.array([2, 3, 4, 5])\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "n_iterations = 1000\n",
        "batch_size = 2\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.zeros(X.shape[1])\n",
        "\n",
        "# Test Batch Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
        "output: [float,float]\n",
        "# Test Stochastic Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
        "output: [float, float]\n",
        "# Test Mini-Batch Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
        "output: [float, float]"
      ],
      "metadata": {
        "id": "zJ4NclF-gucf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_KgLpeapgm8_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n",
        "    m = len(y)\n",
        "\n",
        "    for _ in range(n_iterations):\n",
        "        if method == 'batch':\n",
        "            # Calculate the gradient using all data points\n",
        "            predictions = X.dot(weights)\n",
        "            errors = predictions - y\n",
        "            gradient = 2 * X.T.dot(errors) / m\n",
        "            weights = weights - learning_rate * gradient\n",
        "\n",
        "        elif method == 'stochastic':\n",
        "            # Update weights for each data point individually\n",
        "            for i in range(m):\n",
        "                prediction = X[i].dot(weights)\n",
        "                error = prediction - y[i]\n",
        "                gradient = 2 * X[i].T.dot(error)\n",
        "                weights = weights - learning_rate * gradient\n",
        "\n",
        "        elif method == 'mini_batch':\n",
        "            # Update weights using sequential batches of data points without shuffling\n",
        "            for i in range(0, m, batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                predictions = X_batch.dot(weights)\n",
        "                errors = predictions - y_batch\n",
        "                gradient = 2 * X_batch.T.dot(errors) / batch_size\n",
        "                weights = weights - learning_rate * gradient\n",
        "\n",
        "    return weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\n",
        "y = np.array([2, 3, 4, 5])\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "n_iterations = 1000\n",
        "batch_size = 2\n",
        "\n",
        "# Initialize weights\n",
        "weights = np.zeros(X.shape[1])\n",
        "\n",
        "# Test Batch Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n",
        "print(final_weights)\n",
        "\n",
        "# Test Stochastic Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n",
        "print(final_weights)\n",
        "\n",
        "# Test Mini-Batch Gradient Descent\n",
        "final_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n",
        "print(final_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__e-clNg1tQ",
        "outputId": "dda810c3-96c5-41ac-a647-c241fb5594a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.01003164 0.97050576]\n",
            "[1.00000058 0.99999813]\n",
            "[1.0003804  0.99883421]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqvK3-x7hHLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}